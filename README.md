# spellcheck-app

üìò AI-Powered Speaker Notes Spell Checker

A hybrid baseline+LLM spell-checking system optimized for slide speaker notes, built with Clojure / ClojureScript, re-frame, Compojure-API, Swagger, and Docker.

‚∏ª

‚öôÔ∏è Setup Instructions

1. Clone the repository

git clone <[your-repo-url](https://github.com/ramoneees/spellcheck-app)>
‚∏ª

Backend Setup (Clojure + Leiningen)

Requirements
	‚Ä¢	Java 17+
	‚Ä¢	Leiningen
	‚Ä¢	OpenAI API key (optional but required for advanced mode)

Environment Variable

export OPENAI_API_KEY="sk-your-key"

Run via REPL (recommended for dev)

lein repl
user=> (go)   ;; starts Jetty server on port 3000

Run via Leiningen

lein run


‚∏ª

Frontend Setup (ClojureScript + shadow-cljs + re-frame)

Install dependencies

npm install

Start frontend dev server

npx shadow-cljs watch app

Frontend runs at:

http://localhost:8280

Backend runs at:

http://localhost:3000


‚∏ª

Frontend ‚Üí http://localhost:8280
Backend ‚Üí http://localhost:3000/api/swagger

‚∏ª

üß± Architecture Overview

High-Level Flow

Frontend (re-frame)
   ‚Üì sends JSON payload
Backend route (/api/spellcheck)
   ‚Üì decides mode (baseline / AI / auto)
Spellchecker Pipeline
   ‚Üì baseline detector (dictionary + Levenshtein)
   ‚Üì glossary merging (tech + deck-specific)
   ‚Üì heuristics (llm-needed?)
   ‚Üì optional OpenAI call
   ‚Üì suggestions ‚Üí offsets ‚Üí frontend

Core Components
	‚Ä¢	Baseline Spellchecker
	‚Ä¢	Dictionary-based unknown word detection
	‚Ä¢	Levenshtein suggestion generation
	‚Ä¢	Glossary-aware filtering (tech glossary + deck glossary)
	‚Ä¢	LLM Spellchecker
	‚Ä¢	OpenAI model for grammar + clarity + contextual corrections
	‚Ä¢	Custom prompt designed for speaker notes style
	‚Ä¢	Auto Mode
	‚Ä¢	Runs baseline first, then applies heuristics:
	‚Ä¢	Many errors?
	‚Ä¢	Weak candidates?
	‚Ä¢	Very long unknown token?
	‚Ä¢	Long text with no baseline errors?
	‚Ä¢	Calls LLM only when needed ‚Üí cost-efficient
	‚Ä¢	Glossary System
	‚Ä¢	Loads domain terms from CSV (resources/glossaries/glossary.csv)
	‚Ä¢	Merges domain glossary + deck glossary per request
	‚Ä¢	Ensures products, acronyms, and industry terms are never incorrectly ‚Äúfixed‚Äù
	‚Ä¢	Frontend
	‚Ä¢	Notes editor + context editor
	‚Ä¢	Suggestions panel with badges (AI vs Baseline)
	‚Ä¢	Highlighted inline preview using token offsets
	‚Ä¢	Accept / reject flows
	‚Ä¢	Swagger API
	‚Ä¢	Automatic documentation via Compojure-API

‚∏ª

‚úçÔ∏è Written Component

1. Approach and Key Design Decisions

Hybrid architecture

A pure LLM-based spellchecker is powerful but expensive, high-latency, and occasionally hallucinatory.
A pure dictionary-based spellchecker is fast but lacks context and grammar ability.
So we combine both.

Baseline first, AI second

Running the baseline detector before AI gives us:
	‚Ä¢	Fast response for simple typos
	‚Ä¢	Lower OpenAI usage cost
	‚Ä¢	Better reliability (fallbacks work)
	‚Ä¢	Deterministic behavior around glossary terms

Glossary support

Speaker notes almost always contain:
	‚Ä¢	Technical terms
	‚Ä¢	Acronyms
	‚Ä¢	Brand names
	‚Ä¢	Proper nouns

Traditional spellcheckers mark these as incorrect.
LLMs sometimes rewrite them incorrectly.

A glossary:
	‚Ä¢	Prevents false positives
	‚Ä¢	Prevents hallucinated ‚Äúcorrections‚Äù
	‚Ä¢	Helps the model interpret domain-specific language

This is a major product differentiator.

Auto-Mode (llm-needed?)

We implemented heuristics so the system knows when to escalate to AI:
	‚Ä¢	Large texts with no typos ‚Üí grammar case
	‚Ä¢	Ambiguous baseline suggestions ‚Üí LLM
	‚Ä¢	Very long weird words ‚Üí LLM
	‚Ä¢	Many typos ‚Üí LLM

This keeps cost predictable while delivering high accuracy.

‚∏ª

2. Trade-offs Considered

Accuracy vs. Speed
	‚Ä¢	Baseline mode is extremely fast (ms), but limited.
	‚Ä¢	LLM mode is slower but provides grammar, clarity, and contextual corrections.
	‚Ä¢	Auto-mode gives the best balance:
	‚Ä¢	Use baseline for easy wins
	‚Ä¢	Use AI when needed

Cost vs. Quality
	‚Ä¢	Calling the LLM on every request is expensive.
	‚Ä¢	Auto-mode reduces cost by only escalating in difficult cases.
	‚Ä¢	Glossary reduces unnecessary LLM calls because domain terms don‚Äôt trigger suspicion.

Complexity vs. Maintainability
	‚Ä¢	Adding heuristics increases code complexity.
	‚Ä¢	But it dramatically reduces unnecessary LLM traffic.
	‚Ä¢	The system remains easy to extend because components are isolated:
	‚Ä¢	spellcheck.clj ‚Üí logic
	‚Ä¢	ai.clj ‚Üí model call
	‚Ä¢	glossary.clj ‚Üí loader
	‚Ä¢	events.cljs ‚Üí FE behavior

‚∏ª

3. How This Could Be Extended for Production

Scalability
	‚Ä¢	Run backend behind load balancer, scale horizontally.
	‚Ä¢	Add persistent cache of LLM results to reduce repeat cost.
	‚Ä¢	Move dictionary and glossary into real DB or Redis.
	‚Ä¢	Use job queue for long-running LLM calls.

Privacy
	‚Ä¢	PII scrubbing before sending notes to LLM.
	‚Ä¢	On-prem or private LLM model for sensitive customers.
	‚Ä¢	Encryption in transit + at rest.
	‚Ä¢	Add audit logs to see what text was checked.

Offline Support
	‚Ä¢	Baseline mode already supports offline operation.
	‚Ä¢	Deploy a small local model (e.g., Llama 3.1 8B quantized) for offline LLM capability.
	‚Ä¢	Cache glossaries + dictionary locally.

Enterprise Integration
	‚Ä¢	Add authentication (OAuth2 / JWT).
	‚Ä¢	Track usage per team or deck.
	‚Ä¢	Provide API rate limiting and quotas.

‚∏ª

### üåü Future AI Enhancements

#### **1. Speaker Notes Rewrite Mode**
LLM rewrites entire notes to be:
- more concise,
- more natural for speaking,
- rhythmically structured,
- audience-appropriate.

#### **2. Tone & Delivery Analyzer**
AI analyzes notes to detect:
- passive voice,
- overly complex sentences,
- lack of narrative flow,
- missing transitions.

Could suggest improvements like:

> ‚ÄúThis part would benefit from a clearer call-to-action.‚Äù

#### **3. Slide-to-Notes Consistency Checker**
Given slide content (title + bullets + visuals), the AI:
- checks if notes accurately reflect the slide,
- detects missing explanations,
- provides suggestions like:

> ‚ÄúYou mentioned Kafka Streams but the slide is about Schema Registry.‚Äù

#### **4. AI Cost Estimation & Optimization Layer**
Introduce a lightweight cost-estimator that calculates approximate token usage for each LLM request and returns a field like `aiCostEstimate` alongside suggestions. This allows:
- users to understand when and why AI was invoked,
- teams to monitor and optimize LLM spend,
- auto-mode to incorporate cost constraints into its decision-making.

This creates transparency around computational cost and aligns the feature with real-world scalability and budget considerations.

### **Evaluation Strategy**

To evaluate this feature, I would build a curated benchmark of speaker notes paired with ground-truth corrections, covering different scenarios: simple typos, grammar issues, domain-specific glossary terms, and deliberately tricky edge cases. For each model configuration (baseline only, AI only, and auto-mode), I would run the entire benchmark and measure how often the system produces the expected corrections, targeting at least ~95% accuracy on the test set. Concretely, I‚Äôd track precision and recall over suggested changes (how many of the suggestions are actually correct, and how many real errors we miss), as well as user-centric metrics such as the ratio of accepted vs. rejected suggestions and the number of edits needed after using the tool. This combination of offline benchmarking plus behavioral metrics from real usage gives a robust picture of effectiveness: we can confidently compare approaches, tune thresholds, and ensure that improvements to the model or heuristics actually translate into fewer errors and less friction for presenters.
